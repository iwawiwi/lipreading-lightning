{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REF: https://pytorchvideo.org/docs/tutorial_torchhub_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from typing import Dict\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
    "\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import ApplyTransformToKey, ShortSideScale, UniformTemporalSubsample, UniformCropVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# pretrained model\n",
    "model_name = \"slowfast_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommet to download data\n",
    "# !wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# map id to name mapping\n",
    "id_to_name = {v: str(k).replace('\"',\"\") for k, v in class_names.items()}\n",
    "print(id_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input transforms\n",
    "\n",
    "Depend on model selection: \n",
    "\n",
    "- [SlowFast](https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/)\n",
    "- [X3D](https://pytorch.org/hub/facebookresearch_pytorchvideo_x3d/)\n",
    "- [Slow](https://pytorch.org/hub/facebookresearch_pytorchvideo_resnet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for slowfast\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "alpha = 4.0\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting a video frames as a list of tensors\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # perform temporal sampling from the fast pathway\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,                                                                                  # 1st dim\n",
    "            torch.linspace(0, frames.shape[1] - 1, int(frames.shape[1] // alpha)).long(),       # indices, third param must be integer\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform = ApplyTransformToKey(\n",
    "    key = \"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(num_frames),\n",
    "        Lambda(lambda x: x / 255.0),\n",
    "        NormalizeVideo(mean, std),\n",
    "        ShortSideScale(size=side_size),\n",
    "        CenterCropVideo(crop_size),\n",
    "        PackPathway(), # from class ```PackPathway```\n",
    "    ]),\n",
    ")\n",
    "\n",
    "# duration of input clip is also specific to the model\n",
    "clip_duration = (num_frames * sampling_rate) / frames_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load video example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the example video file, uncomment to download\n",
    "# !wget https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example\n",
    "video_path = \"archery.mp4\"\n",
    "\n",
    "# select start and end time of the clip, start should correspond to where action occurs in the video\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "\n",
    "# initialize EncodedVideo\n",
    "encoded_video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# load clip\n",
    "video_data = encoded_video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "# apply transform\n",
    "video_data = transform(video_data)\n",
    "\n",
    "# move inputs to device\n",
    "inputs = video_data[\"video\"]\n",
    "inputs = [i.to(device)[None, ...] for i in inputs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input clip thorugh the model\n",
    "logits = model(inputs)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect top 5 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted class\n",
    "post_act = torch.nn.Softmax(dim=1)\n",
    "prediction = post_act(logits)\n",
    "pred_classes = prediction.topk(k=5).indices\n",
    "\n",
    "# Map the predicted classes to the label names\n",
    "pred_class_names = [id_to_name[i] for i in pred_classes.cpu().numpy()[0]]\n",
    "print(pred_class_names)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dd0912a7cefb2d506aeeb16f751d49908d61cb2cdcc882d6a1ed2ecca85104b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torchlight')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
