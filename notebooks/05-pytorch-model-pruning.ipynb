{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch model pruning\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.conv1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```conv1``` is a $3\\times3$ convolutional layer with $6$ filters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently no buffer is allocated\n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning module's ```weight```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pruning ```weight``` of the module, we get ```weight_masked``` parameters on buffer. Based on the command, we are zero-ing $30\\%$ of ```weight``` in the ```conv1``` module by random factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new ```weight``` now has zero weight based on zero value in ```weight_mask``` and the others is kept as original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because currently we only pruning ```weight``` of the module so far, we only have one pre-hook before forward pass are applied. In the new forward pass, we apply the mask to the original weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning ```bias```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.l1_unstructured(module, name=\"bias\", amount=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are pruining three smallest bias entry by L1 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have parameter ```weight_orig``` and ```bias_orig``` indicating our module ```weight``` and ```bias``` are modified. ```weight_orig``` and ```bias_orig``` store the original weight and bias before pruning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add ```bias_mask``` to the buffers. As you can see, the ```bias_mask``` has three zero value indicating that we are pruning the three smallest bias entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the new bias after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now have two pre-hooks before forward pass are applied. In the new forward pass, we apply the mask to the original weight and original bias too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could further do iterative pruning on the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are further prune ```conv1``` weight (```name=weight```) by using L2 norm (```n=2```) by half amount (```amount=0.5```) of the layer size (```dim=0```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == \"weight\":  # select out the correct hook\n",
    "        break\n",
    "\n",
    "print(list(hook))  # pruning history in the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pre-hook on the ```weight``` parameter now has two consecutive pruning routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notify that in the model ```state_dict()``` now we have additional parameter corresponding to module mask. This can easily be saved by using ```torch.save()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pruning **permanent**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing re-parameterization on ```weight```\n",
    "prune.remove(module, \"weight\")\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notify that now our ```weight``` is no longer named as ```weight_orig``` meaning that we are permanently pruning the ```weight``` of the module. However, our ```bias``` still has data in buffer because we don't make the changes permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning multiple parameters in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = LeNet()\n",
    "for name, module in new_model.named_modules():\n",
    "    # prune 20 percents of connections (weights) in all 2d-conv layers\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)\n",
    "    # prune 40 percents of connections (weights) in all linear layers\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.4)\n",
    "\n",
    "print(dict(new_model.named_buffers()).keys()) # verify all modules have corresponding mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.conv1, \"weight\"),\n",
    "    (model.conv2, \"weight\"),\n",
    "    (model.fc1, \"weight\"),\n",
    "    (model.fc2, \"weight\"),\n",
    "    (model.fc3, \"weight\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparsity original parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_module_sparsity(module):\n",
    "    percentage = 100. * float(torch.sum(module.weight == 0)) / float(module.weight.nelement())\n",
    "    print(\n",
    "        f\"Sparsity in {module.__module__}: {percentage:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_module_sparsity(model.conv1)\n",
    "check_module_sparsity(model.conv2)\n",
    "check_module_sparsity(model.fc1)\n",
    "check_module_sparsity(model.fc2)\n",
    "check_module_sparsity(model.fc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are pruning 20 percent of parameters overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_module_sparsity(model.conv1)\n",
    "check_module_sparsity(model.conv2)\n",
    "check_module_sparsity(model.fc1)\n",
    "check_module_sparsity(model.fc2)\n",
    "check_module_sparsity(model.fc3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torchlight')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2dd0912a7cefb2d506aeeb16f751d49908d61cb2cdcc882d6a1ed2ecca85104b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
